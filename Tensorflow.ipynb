{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0BPtuTfKJT/KE4eNMzIgW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyaparna/Python-DataScience-CookBook/blob/master/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtI4UrY9UawU"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFCd2iL4XZXQ"
      },
      "source": [
        "#### Note: It is possible to bake this tf.nn.softmax in as the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW3SudWYUg9L"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dropout(0.2),\r\n",
        "  tf.keras.layers.Dense(10)]) #the model returns a vector of \"logits\" or \"log-odds\" scores\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGmY_-r4bJ0L"
      },
      "source": [
        "#### Difference between loss and metrics\r\n",
        "* The loss function is that parameter one passes to Keras model.compile which is actually optimized while training the model . \r\n",
        "* metric is used for judging the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GKDQqP_UhAL"
      },
      "source": [
        "### loss functions for Regression\r\n",
        "\r\n",
        "# Last layer of the model should be\r\n",
        "model.add(Dense(1, activation='linear'))\r\n",
        "\r\n",
        "# Mean Squared Error Loss - if the distribution of the target variable is Gaussian/Normal\r\n",
        "model.compile(loss='mean_squared_error', metrics=['mse']) \r\n",
        "\r\n",
        "# Mean Squared Logarithmic Error Loss - When target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error.\r\n",
        "# first calculates the natural logarithm of each of the predicted values, then calculate the mean squared error\r\n",
        "model.compile(loss='mean_squared_logarithmic_error')\r\n",
        "\r\n",
        "# Mean Absolute Error Loss - When distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value. it is more robust to outliers\r\n",
        "# It is calculated as the average of the absolute difference between the actual and predicted values\r\n",
        "model.compile(loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05QAUg0YUhDN"
      },
      "source": [
        "### loss functions for Classification\r\n",
        "\r\n",
        "# Binary Cross-Entropy Loss - It is intended for use with binary classification where the target values are in the set {0, 1}\r\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "# Last layer of the model should be\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# Hinge Loss - It is intended for use with binary classification where the target values are in the set {-1, 1}. primarily developed for use with Support Vector Machine (SVM) models.\r\n",
        "# The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.\r\n",
        "model.compile(loss='hinge')\r\n",
        "# Last layer of the model should be\r\n",
        "model.add(Dense(1, activation='tanh')) # Activation function is tanh as range is [-1,1]\r\n",
        "\r\n",
        "\r\n",
        "# Squared Hinge Loss - It is intended for use with binary classification where the target values are in the set {-1, 1}. \r\n",
        "# If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate.\r\n",
        "# It has the effect of smoothing the surface of the error function and making it numerically easier to work with.\r\n",
        "model.compile(loss='squared_hinge')\r\n",
        "# Last layer of the model should be\r\n",
        "model.add(Dense(1, activation='tanh')) # Activation function is tanh as range is [-1,1]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSmnnXn1ePHW"
      },
      "source": [
        "### Multi-Class Classification Loss Functions\r\n",
        "\r\n",
        "# Last layer of the model should be\r\n",
        "model.add(Dense(n, activation='softmax')) # n (number of nodes) is number of classes. this means that the target variable must be one hot encoded before splitting into train and test and before fitting the model\r\n",
        "\r\n",
        "# Cross-entropy is the default loss function to use for multi-class classification problems where each class is assigned a unique integer value\r\n",
        "model.compile(loss='categorical_crossentropy')\r\n",
        "\r\n",
        "# Sparse Multiclass Cross-Entropy Loss: Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\r\n",
        "# For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\r\n",
        "model.compile(loss='sparse_categorical_crossentropy')\r\n",
        "\r\n",
        "# Kullback Leibler Divergence Loss\r\n",
        "# Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution\r\n",
        "# KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\r\n",
        "\r\n",
        "model.compile(loss='kullback_leibler_divergence')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}