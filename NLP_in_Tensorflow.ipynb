{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP in Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6Sob55m9Q3PFsSs1Dq3i6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyaparna/Python-DataScience-CookBook/blob/master/NLP_in_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRq6ziwBcpzw",
        "outputId": "59fa3d26-951f-475d-cc67-f5f9a9ff07d4"
      },
      "source": [
        "! pip install emoji"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 81kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.6MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix8SGWXZROVX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn6f-mUfbN_J"
      },
      "source": [
        "* https://www.rexegg.com/regex-quickstart.html\n",
        "* https://docs.python.org/3/library/re.html\n",
        "* http://www.pyregex.com/\n",
        "* https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
        "* All Unicode characters https://en.wikipedia.org/wiki/List_of_Unicode_characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-J-IDUQM0W"
      },
      "source": [
        "### Cleaning functions\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ya8o9ZwQNCZ"
      },
      "source": [
        "example_text = \"\"\n",
        "\n",
        "tf.strings.lower(example_text)  # converts example_text to lowercase\n",
        "\n",
        "tf.strings.regex_replace(example_text, '<br />', '') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# string.punctuation contains !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~   # special characters like these are not removed\n",
        "# similar to string.punctuation functions https://docs.python.org/3/library/string.html\n",
        "# re.escape is to escape special characters in above\n",
        "# % is used to point to string.punctuation\n",
        "# Any punctuation attached to present seperately in the string is removed\n",
        "string.punctuation+'®' # to add more characters to string.punctuation\n",
        "tf.strings.regex_replace(example_text,'[%s]' % re.escape(string.punctuation),'') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# To convert/encode unicode characters \n",
        "example_text.encode(\"utf-8\")\n",
        "# To decode the format back to unicode character\n",
        "example_text.dencode(\"utf-8\")\n",
        "\n",
        "# Extract only text data without labels\n",
        "raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# To get vocab size of the vectorize layer\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd4XUXUuP-gV"
      },
      "source": [
        "### https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "* In Text Vectorize layer: Each word is given a unique number and encoding of the sentence of the sentence is based on that. Transforms strings into vocab indices\n",
        "* This Vectorize layer is passed through the keras embedding layer to get embedding vectors for those words. Embedding vector is learnt during training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ-fBZYxP4So"
      },
      "source": [
        "### Tensorflow Notebooks\n",
        "\"\"\" Data is extracted from url into folders. Then data is passed through 'tf.keras.preprocessing.text_dataset_from_directory' function to \n",
        "prepare data into suitable format \"\"\"\n",
        "\n",
        "# Function for a sneak peek into the data </br>\n",
        "for text_batch, label_batch in raw_train_ds.take(1): \n",
        "  for i in range(3): \n",
        "    print(\"Review\", text_batch.numpy()[i]) \n",
        "    print(\"Label\", label_batch.numpy()[i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLXQVMd8RB4L",
        "outputId": "9422d6df-b1fa-4cc7-edb6-fe54fe1ca364"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', '')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')\n",
        "raw_data_sample = 'ത'+'\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
        "custom_standardization(raw_data_sample)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xe0\\xb4\\xa4pandemonium is a horror\\xc2\\xae movie spoof  that comes off more stupid than funnybelieve me when i tell you i love comedies especially comedy spoofs airplane the naked gun trilogy blazing saddles high anxiety and spaceballs are some of my favorite comedies that spoof a particular genre pandemonium is not up there with those films most of the scenes in this movie had me sitting there in stunned silence because the movie wasnt all that funny there are a few laughs in the film but when you watch a comedy you expect to laugh a lot more than a few times and thats all this film has going for it geez scream had more laughs than this film and that was more of a horror film how bizarre is that12 out of four'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJlVDEO3oImm"
      },
      "source": [
        "# ADAPT the vectorize_layer on only Text part of train dat (remove label)\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,  # Default is 'lower_and_strip_punctuation'.\n",
        "    max_tokens=None,  # no cap on number of tokens\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call adapt\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lqZhvbpeW-"
      },
      "source": [
        "# To retrieve a batch (of 32 reviews and labels) from the dataset: loop is added to the existing code\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "for i in range(len(text_batch)):\n",
        "  first_review, first_label = text_batch[i], label_batch[i]\n",
        "  print(\"Review\", first_review)\n",
        "  print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "  print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO5QqSueqcSj"
      },
      "source": [
        "# To get word associated with a unique number\n",
        "vectorize_layer.get_vocabulary()[1287]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_bnIsIejse"
      },
      "source": [
        "text_col = data['feature_text'].values\n",
        "target_col = data['target'].values\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=None,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=None)\n",
        "\n",
        "vectorize_layer.adapt(text_col) \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  vectorize_layer, # Added Vectorize layer here\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  tf.keras.layers.LSTM(64),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  Dense(1,activation='softplus')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "              metrics=[tf.keras.metrics.MeanAbsolutePercentageError(), tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erHhXAxXejpV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH0AtmOLejmL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRwGrTxeejjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk5qlQTgZE08",
        "outputId": "e49b2029-635e-44da-89e4-ebeeef59caa7"
      },
      "source": [
        "print(raw_data_sample)\n",
        "\n",
        "print(('ത'+raw_data_sample).encode(\"utf-8\"))\n",
        "\n",
        "print(raw_data_sample.encode(\"utf-8\").decode(\"utf-8\"))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
            "b'\\xe0\\xb4\\xa4\"Pandemonium\" -is a horror\\xc2\\xae movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvSpqgY-RC6G",
        "outputId": "2ba77aa1-f9b8-4216-a4c3-759e04f84fa4"
      },
      "source": [
        "re.findall(r'[%s]', raw_data_sample)\n",
        "print(re.findall(r'[%s] % re.escape(string.punctuation)', raw_data_sample))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uTZpwwVfB_U"
      },
      "source": [
        " \"résumé\".encode(\"utf-8\")\n",
        "\n",
        " \"El Niño\".encode(\"utf-8\")\n",
        "\n",
        "\n",
        " b\"r\\xc3\\xa9sum\\xc3\\xa9\".decode(\"utf-8\")\n",
        "'résumé'\n",
        ".decode(\"utf-8\")\n",
        "'El Niño'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l0TIGwO8YUp_",
        "outputId": "e617804f-b0a7-4caa-83e2-6330bafc65a3"
      },
      "source": [
        "import emoji\n",
        "\n",
        "def extract_emojis(s):\n",
        "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\n",
        "\n",
        "extract_emojis(raw_data_sample)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHbSF8z7Y0m5",
        "outputId": "39ff3db1-64f2-4742-a3ef-266c4d9a0da8"
      },
      "source": [
        "pattern = re.compile(\"d\")\n",
        "pattern.search(\"dog\") "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='d'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}