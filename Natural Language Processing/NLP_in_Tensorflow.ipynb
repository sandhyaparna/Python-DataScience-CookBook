{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP in Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCuU41wAm0spx1/Bd3z9E6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyaparna/Python-DataScience-CookBook/blob/master/Natural%20Language%20Processing/NLP_in_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRq6ziwBcpzw",
        "outputId": "59fa3d26-951f-475d-cc67-f5f9a9ff07d4"
      },
      "source": [
        "! pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 81kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.6MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dElv8YmDE_Jq"
      },
      "source": [
        "!pip install tensorflow==2.3.2\n",
        "!pip install tensorflow-text==2.3.0  # tensorflow_text\n",
        "!pip install tfx==0.26.1 \n",
        "!pip install tf-models-official==2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-pkgVxrxql4"
      },
      "source": [
        "* Use clothing data set to develop a pre-trained model \n",
        "  * Amazon product descriptions in metadata files: http://deepyeti.ucsd.edu/jianmo/amazon/index.html\n",
        "  * Read data from json files: http://jmcauley.ucsd.edu/data/amazon/ ; https://nijianmo.github.io/amazon/index.html\n",
        "\n",
        "* clothing/fashion word embeddings https://making.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
        "* https://github.com/mvasil/fashion-compatibility\n",
        "* general product descriptions data: https://data.world/promptcloud/fashion-products-on-amazon-com/workspace/file?filename=amazon_co-ecommerce_sample.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix8SGWXZROVX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn6f-mUfbN_J"
      },
      "source": [
        "* https://www.rexegg.com/regex-quickstart.html\n",
        "* https://docs.python.org/3/library/re.html\n",
        "* http://www.pyregex.com/\n",
        "* https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
        "* All Unicode characters https://en.wikipedia.org/wiki/List_of_Unicode_characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-J-IDUQM0W"
      },
      "source": [
        "### Cleaning functions\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ya8o9ZwQNCZ"
      },
      "source": [
        "example_text = \"\"\n",
        "\n",
        "tf.strings.lower(example_text)  # converts example_text to lowercase\n",
        "\n",
        "tf.strings.regex_replace(example_text, '<br />', '') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# string.punctuation contains !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~   # special characters like these are not removed\n",
        "# similar to string.punctuation functions https://docs.python.org/3/library/string.html\n",
        "# re.escape is to escape special characters in above\n",
        "# % is used to point to string.punctuation\n",
        "# Any punctuation attached to present seperately in the string is removed\n",
        "string.punctuation+'®' # to add more characters to string.punctuation\n",
        "tf.strings.regex_replace(example_text,'[%s]' % re.escape(string.punctuation),'') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# To convert/encode unicode characters \n",
        "example_text.encode(\"utf-8\")\n",
        "# To decode the format back to unicode character\n",
        "example_text.dencode(\"utf-8\")\n",
        "\n",
        "# Extract only text data without labels\n",
        "raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# To get vocab size of the vectorize layer\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n",
        "\n",
        "# Identify all unicode characters\n",
        "[^\\u0000-\\u007e]+\n",
        "\n",
        "# Retains only numbers or letters\n",
        "Df['Text_col'].apply(lambda x: re.findall(\"\\w+\",str(x)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eBmLZ_exL14",
        "outputId": "0c4b5b45-6897-4e41-e590-683c4a440cb2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "hub_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
        "embed = hub.KerasLayer(hub_url)\n",
        "embeddings = embed([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
        "print(embeddings.shape, embeddings.dtype)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 128) <dtype: 'float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98mmjDzxEuB1"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uH6rYlvF3uA"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "# from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpF66S1EtPW",
        "outputId": "d526e18a-2665-40a8-9dfc-eda3e60c45f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# BERT pre-process model\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "# BERT pre-trained model\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "\n",
        "text_test = ['this is such an amazing movie']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "# input_word_ids contains numerical ids for each tokenized input\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "\n",
        "# Max sequence length for any BERT model = 512\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')  # sequence length = 128\n",
        "\n",
        "# Inputs are tokenized using vocab file\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')  # Tokenized sequence\n",
        "\n",
        "# 1 for useful tokens, 0 for padding\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')  # Binary encoding to differentiate padded tokens\n",
        "\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')  # indicates whether the token i belongs to sentence 1 (value 0) or 2 (value 1)\n",
        "\n",
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "# outputs=[pooled_output, sequence_output])\n",
        "\n",
        "# sequence_output for 1 sentence is 512 dimensions for 128 tokens (as 128 is max sequence length)\n",
        "\n",
        "# Each position outputs a vector of 512. Outputs classification CLS token\n",
        "# We are interested in only CLS token to pass through further keras layer for classification/regression.\n",
        "# pooled_output shape of CLS token is [batch_size, 512]  \n",
        "\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  102    0    0    0    0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 0 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.94622993  0.96978396  0.13774446  0.27936164 -0.3334526   0.34829125\n",
            "  0.94912225 -0.9515633  -0.00178645 -0.93107945 -0.04124452 -0.97975695]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[-0.34628797  0.25210583  0.5756581  ... -0.15549679  0.472016\n",
            "   0.37473977]\n",
            " [-0.5864688   0.31059366 -0.27019346 ...  0.5594992  -0.23402517\n",
            "   0.92162156]\n",
            " [-0.87764287  0.43894887 -0.61759305 ...  0.0450628  -0.34966043\n",
            "   0.43843132]\n",
            " ...\n",
            " [-0.3622594  -0.2679375  -0.04834296 ...  0.40202618  0.5345558\n",
            "   0.5671515 ]\n",
            " [-1.0682071  -0.670603    0.14062242 ...  0.12432958  0.40765905\n",
            "   0.7977506 ]\n",
            " [-0.42452416 -0.14097393  0.02649997 ...  0.1602087   0.8911413\n",
            "  -0.0625942 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-ZrdcQbqX8"
      },
      "source": [
        "### Activation functions for Regression(y>0) :\n",
        "* softplus\n",
        "* ReLU\n",
        "* Exponential\n",
        "* Piece-wise linear\n",
        "\n",
        "### loss functions\n",
        "* Region Proposal Network box\n",
        "* object Localization Network RPN centerness "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd4XUXUuP-gV"
      },
      "source": [
        "### https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "* In Text Vectorize layer: Each word is given a unique number and encoding of the sentence of the sentence is based on that. Transforms strings into vocab indices\n",
        "* This Vectorize layer is passed through the keras embedding layer to get embedding vectors for those words. Embedding vector is learnt during training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ-fBZYxP4So"
      },
      "source": [
        "### Tensorflow Notebooks\n",
        "\"\"\" Data is extracted from url into folders. Then data is passed through 'tf.keras.preprocessing.text_dataset_from_directory' function to \n",
        "prepare data into suitable format \"\"\"\n",
        "\n",
        "# Function for a sneak peek into the data </br>\n",
        "for text_batch, label_batch in raw_train_ds.take(1): \n",
        "  for i in range(3): \n",
        "    print(\"Review\", text_batch.numpy()[i]) \n",
        "    print(\"Label\", label_batch.numpy()[i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLXQVMd8RB4L",
        "outputId": "9422d6df-b1fa-4cc7-edb6-fe54fe1ca364"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', '')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')\n",
        "\n",
        "# Added removal of unicode characters to the above code  \n",
        "def custom_standardization_check(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  linebreaks = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  stripped_html = tf.strings.regex_replace(linebreaks,\n",
        "                                  '[%s]' % re.escape(string.punctuation),' ')\n",
        "  return tf.strings.regex_replace(stripped_html, '[^\\u0000-\\u007e]+', ' ')\n",
        "\n",
        "raw_data_sample = 'ത'+'\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
        "custom_standardization(raw_data_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xe0\\xb4\\xa4pandemonium is a horror\\xc2\\xae movie spoof  that comes off more stupid than funnybelieve me when i tell you i love comedies especially comedy spoofs airplane the naked gun trilogy blazing saddles high anxiety and spaceballs are some of my favorite comedies that spoof a particular genre pandemonium is not up there with those films most of the scenes in this movie had me sitting there in stunned silence because the movie wasnt all that funny there are a few laughs in the film but when you watch a comedy you expect to laugh a lot more than a few times and thats all this film has going for it geez scream had more laughs than this film and that was more of a horror film how bizarre is that12 out of four'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJlVDEO3oImm"
      },
      "source": [
        "# ADAPT the vectorize_layer on only Text part of train dat (remove label)\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,  # Default is 'lower_and_strip_punctuation'.\n",
        "    max_tokens=None,  # no cap on number of tokens\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call adapt\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lqZhvbpeW-"
      },
      "source": [
        "# To retrieve a batch (of 32 reviews and labels) from the dataset: loop is added to the existing code\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "for i in range(len(text_batch)):\n",
        "  first_review, first_label = text_batch[i], label_batch[i]\n",
        "  print(\"Review\", first_review)\n",
        "  print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "  print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO5QqSueqcSj"
      },
      "source": [
        "# To get word associated with a unique number\n",
        "vectorize_layer.get_vocabulary()[1287]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_bnIsIejse"
      },
      "source": [
        "text_col = data['feature_text'].values\n",
        "target_col = data['target'].values\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=None,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=None)\n",
        "\n",
        "vectorize_layer.adapt(text_col) \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  vectorize_layer, # Added Vectorize layer here\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  tf.keras.layers.LSTM(64),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  Dense(1,activation='softplus')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "              metrics=[tf.keras.metrics.MeanAbsolutePercentageError(), tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNyUShZSiojz"
      },
      "source": [
        "# Extract all unciode characters from text column and then concatenate all characters in differnt rows of a column as a string\n",
        "df['unicode_chars'] = df['Text_col'].apply(lambda x: re.findall('[^\\u0000-\\u007e]+',str(x)))\n",
        "df['unicode_chars'] = [','.join(map(str, l)) for l in df['unicode_chars']]  # concatenate rows as a string\n",
        "unicode_chars_uq = set(', '.join(df['unicode_chars'].apply(lambda x: str(x))))  # to get distinct characters\n",
        "unicode_chars_uq\n",
        "# result = {' ', ',', '\\xa0', '®', '°', 'º', '–', '—', '’', '”', '™', '�'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk5qlQTgZE08",
        "outputId": "e49b2029-635e-44da-89e4-ebeeef59caa7"
      },
      "source": [
        "print(raw_data_sample)\n",
        "print(('ത'+raw_data_sample).encode(\"utf-8\"))\n",
        "print(raw_data_sample.encode(\"utf-8\").decode(\"utf-8\"))\n",
        "\n",
        "# Identify all unicode characters\n",
        "df['text_col'].apply(lambda x: re.findall('[^\\u0000-\\u007e]+',str(x)))\n",
        "\n",
        "string_check = \"sandy� � ® ° 200gsm (Bright Lycra)\"\n",
        "print(string_check.encode('ascii', 'replace'))\n",
        "Df['text_col'].apply(lambda x: x.encode('ascii', 'replace'))\n",
        "[s.encode('ascii', 'ignore') for s in Df['text_col']]\n",
        "\n",
        "print(string_check.encode('ascii', 'ignore'))\n",
        "string_check\n",
        "\n",
        "# Tensorflow \n",
        "# converted � to ? {' ', ',', '\\xa0', '®', '°', 'º', '–', '—', '’', '”', '™', '�'}\n",
        "string_check = tf.constant(u\"sandy� � ® ° 200gsm (Bright Lycra)\".encode('ascii','ignore'))\n",
        "print(string_check)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
            "b'\\xe0\\xb4\\xa4\"Pandemonium\" -is a horror\\xc2\\xae movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erHhXAxXejpV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH0AtmOLejmL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRwGrTxeejjK"
      },
      "source": [
        "self.embedding = nn.Embedding(one_hot_dim, embedding_dim)\n",
        "        self.maxpool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool1d(kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(in_features=size_after_conv, out_features=output_dim)\n",
        "\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvSpqgY-RC6G",
        "outputId": "2ba77aa1-f9b8-4216-a4c3-759e04f84fa4"
      },
      "source": [
        "re.findall(r'[%s]', raw_data_sample)\n",
        "print(re.findall(r'[%s] % re.escape(string.punctuation)', raw_data_sample))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uTZpwwVfB_U"
      },
      "source": [
        " \"résumé\".encode(\"utf-8\")\n",
        "\n",
        " \"El Niño\".encode(\"utf-8\")\n",
        "\n",
        "\n",
        " b\"r\\xc3\\xa9sum\\xc3\\xa9\".decode(\"utf-8\")\n",
        "'résumé'\n",
        ".decode(\"utf-8\")\n",
        "'El Niño'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l0TIGwO8YUp_",
        "outputId": "e617804f-b0a7-4caa-83e2-6330bafc65a3"
      },
      "source": [
        "import emoji\n",
        "\n",
        "def extract_emojis(s):\n",
        "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\n",
        "\n",
        "extract_emojis(raw_data_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHbSF8z7Y0m5",
        "outputId": "39ff3db1-64f2-4742-a3ef-266c4d9a0da8"
      },
      "source": [
        "pattern = re.compile(\"d\")\n",
        "pattern.search(\"dog\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='d'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}