{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP in Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBp/I5gw7vgaIPQ0h1NlcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyaparna/Python-DataScience-CookBook/blob/master/Natural%20Language%20Processing/NLP_in_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dElv8YmDE_Jq"
      },
      "source": [
        "!pip install tensorflow==2.3.2\n",
        "!pip install tensorflow-text==2.3.0  # tensorflow_text\n",
        "!pip install tfx==0.26.1 \n",
        "!pip install tf-models-official==2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-pkgVxrxql4"
      },
      "source": [
        "* Use clothing data set to develop a pre-trained model \n",
        "  * Amazon product descriptions in metadata files: http://deepyeti.ucsd.edu/jianmo/amazon/index.html\n",
        "  * Read data from json files: http://jmcauley.ucsd.edu/data/amazon/ ; https://nijianmo.github.io/amazon/index.html\n",
        "\n",
        "* clothing/fashion word embeddings https://making.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
        "* https://github.com/mvasil/fashion-compatibility\n",
        "* general product descriptions data: https://data.world/promptcloud/fashion-products-on-amazon-com/workspace/file?filename=amazon_co-ecommerce_sample.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix8SGWXZROVX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn6f-mUfbN_J"
      },
      "source": [
        "* https://www.rexegg.com/regex-quickstart.html\n",
        "* https://docs.python.org/3/library/re.html\n",
        "* http://www.pyregex.com/\n",
        "* https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
        "* All Unicode characters https://en.wikipedia.org/wiki/List_of_Unicode_characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-J-IDUQM0W"
      },
      "source": [
        "### Cleaning functions\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ya8o9ZwQNCZ"
      },
      "source": [
        "example_text = \"\"\n",
        "\n",
        "tf.strings.lower(example_text)  # converts example_text to lowercase\n",
        "\n",
        "tf.strings.regex_replace(example_text, '<br />', '') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# string.punctuation contains !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~   # special characters like these are not removed\n",
        "# similar to string.punctuation functions https://docs.python.org/3/library/string.html\n",
        "# re.escape is to escape special characters in above\n",
        "# % is used to point to string.punctuation\n",
        "# Any punctuation attached to present seperately in the string is removed\n",
        "string.punctuation+'Â®' # to add more characters to string.punctuation\n",
        "tf.strings.regex_replace(example_text,'[%s]' % re.escape(string.punctuation),'') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# To convert/encode unicode characters \n",
        "example_text.encode(\"utf-8\")\n",
        "# To decode the format back to unicode character\n",
        "example_text.dencode(\"utf-8\")\n",
        "\n",
        "# Extract only text data without labels\n",
        "raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# To get vocab size of the vectorize layer\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n",
        "\n",
        "# Identify all unicode characters\n",
        "[^\\u0000-\\u007e]+\n",
        "\n",
        "# Retains only numbers or letters\n",
        "Df['Text_col'].apply(lambda x: re.findall(\"\\w+\",str(x)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eBmLZ_exL14",
        "outputId": "a66092de-78a8-4b7c-bb55-a36371740c38"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "hub_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
        "embed = hub.KerasLayer(hub_url)\n",
        "embeddings = embed([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
        "print(embeddings.shape, embeddings.dtype)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 128) <dtype: 'float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98mmjDzxEuB1"
      },
      "source": [
        "### BERT\n",
        "* Difference between various BERT pre-trained models: https://tfhub.dev/google/collections/bert/1\n",
        "* L corresponds to number of hidden layers\n",
        "* H is hidden size\n",
        "* A is Attention heads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uH6rYlvF3uA"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "# from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDpF66S1EtPW",
        "outputId": "d526e18a-2665-40a8-9dfc-eda3e60c45f3"
      },
      "source": [
        "# BERT pre-process model\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "# BERT pre-trained model\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "\n",
        "text_test = ['this is such an amazing movie']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "# input_word_ids contains numerical ids for each tokenized input\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "\n",
        "# Max sequence length for any BERT model = 512\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')  # sequence length = 128\n",
        "\n",
        "# Inputs are tokenized using vocab file\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')  # Tokenized sequence\n",
        "\n",
        "# 1 for useful tokens, 0 for padding\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')  # Binary encoding to differentiate padded tokens\n",
        "\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')  # indicates whether the token i belongs to sentence 1 (value 0) or 2 (value 1)\n",
        "\n",
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "# outputs=[pooled_output, sequence_output])\n",
        "\n",
        "# sequence_output for 1 sentence is 512 dimensions for 128 tokens (as 128 is max sequence length)\n",
        "\n",
        "# Each position outputs a vector of 512. Outputs classification CLS token\n",
        "# We are interested in only CLS token to pass through further keras layer for classification/regression.\n",
        "# pooled_output shape of CLS token is [batch_size, 512]  \n",
        "\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  102    0    0    0    0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 0 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.94622993  0.96978396  0.13774446  0.27936164 -0.3334526   0.34829125\n",
            "  0.94912225 -0.9515633  -0.00178645 -0.93107945 -0.04124452 -0.97975695]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[-0.34628797  0.25210583  0.5756581  ... -0.15549679  0.472016\n",
            "   0.37473977]\n",
            " [-0.5864688   0.31059366 -0.27019346 ...  0.5594992  -0.23402517\n",
            "   0.92162156]\n",
            " [-0.87764287  0.43894887 -0.61759305 ...  0.0450628  -0.34966043\n",
            "   0.43843132]\n",
            " ...\n",
            " [-0.3622594  -0.2679375  -0.04834296 ...  0.40202618  0.5345558\n",
            "   0.5671515 ]\n",
            " [-1.0682071  -0.670603    0.14062242 ...  0.12432958  0.40765905\n",
            "   0.7977506 ]\n",
            " [-0.42452416 -0.14097393  0.02649997 ...  0.1602087   0.8911413\n",
            "  -0.0625942 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRRaVmqd6MpC",
        "outputId": "0da2293e-83cc-4d4a-d458-09aef8d721bf"
      },
      "source": [
        "import tensorflow_text as text\n",
        "\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "# BERT pre-trained model\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "feature_vec_1 = bert_model(bert_preprocess_model([\"3/4 sleeve\"]))[\"pooled_output\"]\n",
        "feature_vec_2 = bert_model(bert_preprocess_model([\"3 4 sleeve\"]))[\"pooled_output\"]\n",
        "\n",
        "cosine_similarity(feature_vec_1, feature_vec_2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337ac9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337ac9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337a10e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337a10e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 12 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7368c2dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 12 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7368c2dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zRClgA-8LRG",
        "outputId": "72ee95d3-0783-4cf4-cf16-11ecb92e0550"
      },
      "source": [
        "import tensorflow_text as text\n",
        "\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "# BERT pre-trained model\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cosine_similarity_fn(text1,text2):\n",
        "  feature_vec_1 = bert_model(bert_preprocess_model([text1]))[\"pooled_output\"]\n",
        "  feature_vec_2 = bert_model(bert_preprocess_model([text2]))[\"pooled_output\"]\n",
        "  return 'similarity of' , text1,'and', text2, cosine_similarity(feature_vec_1, feature_vec_2)\n",
        "\n",
        "print(cosine_similarity_fn(\"3/4 sleeve\",\"3 4 sleeve\"))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 26 calls to <function recreate_function.<locals>.restored_function_body at 0x7f92611f84d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 26 calls to <function recreate_function.<locals>.restored_function_body at 0x7f92611f84d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 27 calls to <function recreate_function.<locals>.restored_function_body at 0x7f925f297200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 27 calls to <function recreate_function.<locals>.restored_function_body at 0x7f925f297200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 28 calls to <function recreate_function.<locals>.restored_function_body at 0x7f92611f84d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 28 calls to <function recreate_function.<locals>.restored_function_body at 0x7f92611f84d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('similarity of', '3/4 sleeve', 'and', '3 4 sleeve', array([[0.9320507]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kNzgrj2qpfr"
      },
      "source": [
        "# {' ',\n",
        "#  '!',\n",
        "#  '\"',\n",
        "#  '#',\n",
        "#  '$',\n",
        "#  '%',\n",
        "#  '&',\n",
        "#  \"'\",\n",
        "#  '(',\n",
        "#  ')',\n",
        "#  '*',\n",
        "#  '+',\n",
        "#  ',',\n",
        "#  '-',\n",
        "#  '.',\n",
        "#  '/',\n",
        "#  ':',\n",
        "#  ';',\n",
        "#  '<',\n",
        "#  '=',\n",
        "#  '>',\n",
        "#  '?',\n",
        "#  '_'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0f7whzoqPe2"
      },
      "source": [
        "# \"!\" - can be left as is\n",
        "# '\"' - eg: 4\" seem; \n",
        "# ? needs to be removed\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvQMhqQsrQmj",
        "outputId": "1f549d56-95cb-4608-cc6b-9285b569f1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "raw_text = r'I have a 15 \" laptop, 4 \" seam \"tide plus\"'\n",
        "re.findall(r'(.....)\"(.......)',raw_text)\n",
        "# re.findall(r'AV', 'AV Analytics Vidhya AV')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a 15 ', ' laptop'), ('seam ', 'tide pl')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSmZLfCosZ3H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGZPp7EosNR-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZIKnCHdGBzC",
        "outputId": "af9ede47-7fdc-4f62-d749-996aa5c95243"
      },
      "source": [
        "print(cosine_similarity_fn(\"don't\",\"do not\"))\n",
        "print(cosine_similarity_fn(\"they're\",\"they are\"))\n",
        "print(cosine_similarity_fn(\"it's\",\"it is\"))\n",
        "print(cosine_similarity_fn(\"it?s\",\"it is\"))\n",
        "print(cosine_similarity_fn(\"easy-to-layer\",\"easy to layer\"))\n",
        "print(cosine_similarity_fn(\"easy-to-layer\",\"easy to layer\"))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('similarity of', \"don't\", 'and', 'do not', array([[0.9083276]], dtype=float32))\n",
            "('similarity of', \"they're\", 'and', 'they are', array([[0.8668688]], dtype=float32))\n",
            "('similarity of', \"it's\", 'and', 'it is', array([[0.76479155]], dtype=float32))\n",
            "('similarity of', 'it?s', 'and', 'it is', array([[0.65618396]], dtype=float32))\n",
            "('similarity of', 'easy-to-layer', 'and', 'easy to layer', array([[0.89126873]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KkUWaj0Aq2q"
      },
      "source": [
        "def cosine_similarity_fn(text1,text2):\n",
        "  feature_vec_1 = bert_model(bert_preprocess_model([text1]))[\"pooled_output\"]\n",
        "  feature_vec_2 = bert_model(bert_preprocess_model([text2]))[\"pooled_output\"]\n",
        "  return 'similarity of' , text1,'and', text2, cosine_similarity(feature_vec_1, feature_vec_2)\n",
        "  # return \"similarity of %s and %s\" % (text1,text2), text1, \" and \", text2, cosine_similarity(feature_vec_1, feature_vec_2)\n",
        "# cosine_similarity_fn(\"3/4 sleeve\",\"3 4 sleeve\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uiPumczER9c"
      },
      "source": [
        "def preprocessing_fn(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  linebreaks = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  stripped_html = tf.strings.regex_replace(linebreaks,\n",
        "                                  '[%s]' % re.escape(string.punctuation),'')\n",
        "  return tf.strings.regex_replace(stripped_html, '[^\\u0000-\\u007e]+', '')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V41Sk_K6ENuY",
        "outputId": "3c7f32f5-aebb-4438-e506-7e04f4ca51f4"
      },
      "source": [
        "print(cosine_similarity_fn(preprocessing_fn(\" Standard size 1.7 fl oz/ 30 ml.\"),\" Standard size 1.7 fl oz/ 30 ml.\"))\n",
        "print(cosine_similarity_fn(\" Standard size 1.7 fl oz  30 ml.\",\" Standard size 1.7 fl oz/ 30 ml.\"))\n",
        "print(cosine_similarity_fn(\"don't\",\"do not\"))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('similarity of ', <tf.Tensor: shape=(), dtype=string, numpy=b' standard size 17 fl oz 30 ml'>, ' and ', ' Standard size 1.7 fl oz/ 30 ml.', array([[0.6451631]], dtype=float32))\n",
            "('similarity of ', ' Standard size 1.7 fl oz  30 ml.', ' and ', ' Standard size 1.7 fl oz/ 30 ml.', array([[0.9953242]], dtype=float32))\n",
            "('similarity of ', \"don't\", ' and ', 'do not', array([[0.9083276]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W00MreBS-ylQ",
        "outputId": "2e6ab98f-9d7e-4833-9cda-107c954f340e"
      },
      "source": [
        "print(cosine_similarity_fn(\"3/4 sleeve\",\"3 4 sleeve\"))\n",
        "print(cosine_similarity_fn(\"3/4 sleeve\",\"3 by 4 sleeve\"))\n",
        "\n",
        "\n",
        "print(cosine_similarity_fn(\"3 Â®\",\"3 \\xc2\\xae\"))\n",
        "print(cosine_similarity_fn(\"3Â®\",\"3\\xc2\\xae\"))\n",
        "print(cosine_similarity_fn(\"3Â®\",\"3\"))\n",
        "print(cosine_similarity_fn(\"3 Â®\",\"3Â®\"))\n",
        "print(cosine_similarity_fn(\"A/B\",\"A by B\"))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('similarity of ', '3/4 sleeve', ' and ', '3 4 sleeve', array([[0.9320507]], dtype=float32))\n",
            "('similarity of ', '3/4 sleeve', ' and ', '3 by 4 sleeve', array([[0.96390617]], dtype=float32))\n",
            "('similarity of ', '3 Â®', ' and ', '3 ÃÂ®', array([[0.87759125]], dtype=float32))\n",
            "('similarity of ', '3Â®', ' and ', '3ÃÂ®', array([[0.9169636]], dtype=float32))\n",
            "('similarity of ', '3Â®', ' and ', '3', array([[0.8828571]], dtype=float32))\n",
            "('similarity of ', '3 Â®', ' and ', '3Â®', array([[0.8762013]], dtype=float32))\n",
            "('similarity of ', 'A/B', ' and ', 'A by B', array([[0.8666401]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPV7HaEqCrvF",
        "outputId": "7c446405-55ff-435f-c1c9-588affe28208"
      },
      "source": [
        "print(\"Â®\".encode())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\\xc2\\xae'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdHmx0dm-ydc",
        "outputId": "752c74ba-687e-407c-9902-7d696520a779"
      },
      "source": [
        "string_check = \"ï¿½ ï¿½ 3Â® Â° 200gsm (Bright Lycra)\"\n",
        "string_check.encode(\"utf-8\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'\\xef\\xbf\\xbd \\xef\\xbf\\xbd 3\\xc2\\xae \\xc2\\xb0 200gsm (Bright Lycra)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJxI_20e-xyK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OVOB6Kf6Yu9"
      },
      "source": [
        "text_test = ['Â®!#$%&()*+,-./:;<=>?@[\\]^_`{|}~Â®'] \n",
        "# text_test = ['\\xef\\xbf\\xbd'] #ï¿½"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-ZrdcQbqX8"
      },
      "source": [
        "### Activation functions for Regression(y>0) :\n",
        "* softplus\n",
        "* ReLU\n",
        "* Exponential\n",
        "* Piece-wise linear\n",
        "\n",
        "### loss functions\n",
        "* Region Proposal Network box\n",
        "* object Localization Network RPN centerness "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd4XUXUuP-gV"
      },
      "source": [
        "### https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "* In Text Vectorize layer: Each word is given a unique number and encoding of the sentence of the sentence is based on that. Transforms strings into vocab indices\n",
        "* This Vectorize layer is passed through the keras embedding layer to get embedding vectors for those words. Embedding vector is learnt during training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ-fBZYxP4So"
      },
      "source": [
        "### Tensorflow Notebooks\n",
        "\"\"\" Data is extracted from url into folders. Then data is passed through 'tf.keras.preprocessing.text_dataset_from_directory' function to \n",
        "prepare data into suitable format \"\"\"\n",
        "\n",
        "# Function for a sneak peek into the data </br>\n",
        "for text_batch, label_batch in raw_train_ds.take(1): \n",
        "  for i in range(3): \n",
        "    print(\"Review\", text_batch.numpy()[i]) \n",
        "    print(\"Label\", label_batch.numpy()[i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLXQVMd8RB4L",
        "outputId": "9422d6df-b1fa-4cc7-edb6-fe54fe1ca364"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', '')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')\n",
        "\n",
        "# Added removal of unicode characters to the above code  \n",
        "def custom_standardization_check(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  linebreaks = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  stripped_html = tf.strings.regex_replace(linebreaks,\n",
        "                                  '[%s]' % re.escape(string.punctuation),' ')\n",
        "  return tf.strings.regex_replace(stripped_html, '[^\\u0000-\\u007e]+', ' ')\n",
        "\n",
        "raw_data_sample = 'à´¤'+'\"Pandemonium\" -is a horrorÂ® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
        "custom_standardization(raw_data_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xe0\\xb4\\xa4pandemonium is a horror\\xc2\\xae movie spoof  that comes off more stupid than funnybelieve me when i tell you i love comedies especially comedy spoofs airplane the naked gun trilogy blazing saddles high anxiety and spaceballs are some of my favorite comedies that spoof a particular genre pandemonium is not up there with those films most of the scenes in this movie had me sitting there in stunned silence because the movie wasnt all that funny there are a few laughs in the film but when you watch a comedy you expect to laugh a lot more than a few times and thats all this film has going for it geez scream had more laughs than this film and that was more of a horror film how bizarre is that12 out of four'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJlVDEO3oImm"
      },
      "source": [
        "# ADAPT the vectorize_layer on only Text part of train dat (remove label)\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,  # Default is 'lower_and_strip_punctuation'.\n",
        "    max_tokens=None,  # no cap on number of tokens\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call adapt\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lqZhvbpeW-"
      },
      "source": [
        "# To retrieve a batch (of 32 reviews and labels) from the dataset: loop is added to the existing code\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "for i in range(len(text_batch)):\n",
        "  first_review, first_label = text_batch[i], label_batch[i]\n",
        "  print(\"Review\", first_review)\n",
        "  print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "  print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO5QqSueqcSj"
      },
      "source": [
        "# To get word associated with a unique number\n",
        "vectorize_layer.get_vocabulary()[1287]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_bnIsIejse"
      },
      "source": [
        "text_col = data['feature_text'].values\n",
        "target_col = data['target'].values\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=None,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=None)\n",
        "\n",
        "vectorize_layer.adapt(text_col) \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  vectorize_layer, # Added Vectorize layer here\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  tf.keras.layers.LSTM(64),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  Dense(1,activation='softplus')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "              metrics=[tf.keras.metrics.MeanAbsolutePercentageError(), tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNyUShZSiojz"
      },
      "source": [
        "# Extract all unciode characters from text column and then concatenate all characters in differnt rows of a column as a string\n",
        "df['unicode_chars'] = df['Text_col'].apply(lambda x: re.findall('[^\\u0000-\\u007e]+',str(x)))\n",
        "df['unicode_chars'] = [','.join(map(str, l)) for l in df['unicode_chars']]  # concatenate rows as a string\n",
        "unicode_chars_uq = set(', '.join(df['unicode_chars'].apply(lambda x: str(x))))  # to get distinct characters\n",
        "unicode_chars_uq\n",
        "# result = {' ', ',', '\\xa0', 'Â®', 'Â°', 'Âº', 'â', 'â', 'â', 'â', 'â¢', 'ï¿½'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Roi6vXhRzi-c",
        "outputId": "378e512c-7403-4e67-960b-6c7d381f86d8"
      },
      "source": [
        "string_check = \"sandyï¿½ ï¿½ Â® Â° 200gsm (Bright Lycra)\"\n",
        "string_check.encode(\"utf-8\")\n",
        "# print(string_check.encode('ascii', 'replace'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'sandy\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xc2\\xae \\xc2\\xb0 200gsm (Bright Lycra)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "nk5qlQTgZE08",
        "outputId": "9c041f79-3117-448d-d5c7-adbeb1dd7be3"
      },
      "source": [
        "raw_data_sample = 'à´¤'+'\"Pandemonium\" -is a horrorÂ® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
        "\n",
        "print(raw_data_sample)\n",
        "print(('à´¤'+raw_data_sample).encode(\"utf-8\"))\n",
        "print(raw_data_sample.encode(\"utf-8\").decode(\"utf-8\"))\n",
        "\n",
        "# Identify all unicode characters\n",
        "df['text_col'].apply(lambda x: re.findall('[^\\u0000-\\u007e]+',str(x)))\n",
        "\n",
        "string_check = \"sandyï¿½ ï¿½ Â® Â° 200gsm (Bright Lycra)\"\n",
        "print(string_check.encode('ascii', 'replace'))\n",
        "Df['text_col'].apply(lambda x: x.encode('ascii', 'replace'))\n",
        "[s.encode('ascii', 'ignore') for s in Df['text_col']]\n",
        "\n",
        "print(string_check.encode('ascii', 'ignore'))\n",
        "string_check\n",
        "\n",
        "# Tensorflow \n",
        "# converted ï¿½ to ? {' ', ',', '\\xa0', 'Â®', 'Â°', 'Âº', 'â', 'â', 'â', 'â', 'â¢', 'ï¿½'}\n",
        "string_check = tf.constant(u\"sandyï¿½ ï¿½ Â® Â° 200gsm (Bright Lycra)\".encode('ascii','ignore'))\n",
        "print(string_check)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "à´¤\"Pandemonium\" -is a horrorÂ® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
            "b'\\xe0\\xb4\\xa4\\xe0\\xb4\\xa4\"Pandemonium\" -is a horror\\xc2\\xae movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "à´¤\"Pandemonium\" -is a horrorÂ® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
            "b'sandy? ? ? ? 200gsm (Bright Lycra)'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-751468cb2dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstring_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sandyï¿½ ï¿½ Â® Â° 200gsm (Bright Lycra)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_col'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_col'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erHhXAxXejpV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH0AtmOLejmL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRwGrTxeejjK"
      },
      "source": [
        "self.embedding = nn.Embedding(one_hot_dim, embedding_dim)\n",
        "        self.maxpool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool1d(kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(in_features=size_after_conv, out_features=output_dim)\n",
        "\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvSpqgY-RC6G",
        "outputId": "2ba77aa1-f9b8-4216-a4c3-759e04f84fa4"
      },
      "source": [
        "re.findall(r'[%s]', raw_data_sample)\n",
        "print(re.findall(r'[%s] % re.escape(string.punctuation)', raw_data_sample))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uTZpwwVfB_U"
      },
      "source": [
        " \"rÃ©sumÃ©\".encode(\"utf-8\")\n",
        "\n",
        " \"El NiÃ±o\".encode(\"utf-8\")\n",
        "\n",
        "\n",
        " b\"r\\xc3\\xa9sum\\xc3\\xa9\".decode(\"utf-8\")\n",
        "'rÃ©sumÃ©'\n",
        ".decode(\"utf-8\")\n",
        "'El NiÃ±o'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l0TIGwO8YUp_",
        "outputId": "e617804f-b0a7-4caa-83e2-6330bafc65a3"
      },
      "source": [
        "import emoji\n",
        "\n",
        "def extract_emojis(s):\n",
        "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\n",
        "\n",
        "extract_emojis(raw_data_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHbSF8z7Y0m5",
        "outputId": "39ff3db1-64f2-4742-a3ef-266c4d9a0da8"
      },
      "source": [
        "pattern = re.compile(\"d\")\n",
        "pattern.search(\"dog\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='d'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}