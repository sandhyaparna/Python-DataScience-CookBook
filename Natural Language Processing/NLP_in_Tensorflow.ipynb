{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP in Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGUEoehhTA/9vbkYjq6ciJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyaparna/Python-DataScience-CookBook/blob/master/Natural%20Language%20Processing/NLP_in_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dElv8YmDE_Jq",
        "outputId": "67cfa739-5a47-4da1-b7e9-c31996c53a5e"
      },
      "source": [
        "!pip install tensorflow==2.3.2\n",
        "!pip install tensorflow-text==2.3.0  # tensorflow_text\n",
        "!pip install tfx==0.26.1 \n",
        "!pip install tf-models-official==2.3.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8c/9d150ce34e053432ad412d5627be7417764731e372b9b1746e198c0b8c81/tensorflow-2.3.2-cp37-cp37m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (0.36.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (2.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (0.2.0)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (1.12.1)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.2) (1.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.28.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (54.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.4.1)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, numpy, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed numpy-1.18.5 tensorflow-2.3.2 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/07/af34964fe4769fcd8258e78c778445a87e009c41ce6bedea451736b4ff3e/tensorflow_text-2.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.3.0) (2.3.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (0.12.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (0.36.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (2.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (54.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.28.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (0.4.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text==2.3.0) (3.1.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.3.0\n",
            "Collecting tfx==0.26.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/5e/af1c0057082b6bd489810488c841a9b4ee98ae84314c2671d50688783459/tfx-0.26.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 5.2MB/s \n",
            "\u001b[?25hCollecting tfx-bsl<0.27,>=0.26.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/09/8ca84817cf8683d28377f421dc7e79579914bde2b061afb2e176740479f6/tfx_bsl-0.26.1-cp37-cp37m-manylinux2010_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (1.12.8)\n",
            "Collecting absl-py<0.11,>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (1.15.0)\n",
            "Requirement already satisfied: jinja2<3,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (2.11.3)\n",
            "Collecting pyarrow<0.18,>=0.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/78/dcd7f290cd018581b5c73f6c87e2b004f1161cdf6f55c7b2c87d78174592/pyarrow-0.17.1-cp37-cp37m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8MB 56kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (3.13)\n",
            "Collecting tensorflow-data-validation<0.27,>=0.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/9a/faa0b345ddd6266a36a210b2e23f861757dc2834639940c16a61f66ed404/tensorflow_data_validation-0.26.0-cp37-cp37m-manylinux2010_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 31.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-transform<0.27,>=0.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/22/e3d22f4f7cd9140334b37daa432f3904975d6845c5320bc335b97752e567/tensorflow_transform-0.26.0-py3-none-any.whl (380kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 31.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.27,>=0.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/0f/3d2b92d0fa732059f6fa0cff69cb1227f15de108cbe5995cc0744cfb2e8d/tensorflow_model_analysis-0.26.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 26.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0d/0f0822b418fd51795a4768f35bb17619af51312ca9f5762c80bea34bd7ae/tensorflow_serving_api-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (1.32.0)\n",
            "Collecting ml-metadata<0.27,>=0.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/cb/e6321afca3126feb0d3d24344c5229ebaebd91e94fc9761f392342a0ac17/ml_metadata-0.26.0-cp37-cp37m-manylinux2010_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 25.1MB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]!=2.26.*,<3,>=2.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/e7/d6e5a3786d9a037a38af966bf154bcd6cb3cbea2edffda00cf6c417cc9a2/apache_beam-2.28.0-cp37-cp37m-manylinux2010_x86_64.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 37.9MB/s \n",
            "\u001b[?25hCollecting kubernetes<12,>=10.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/46/d4b9364fcab59b5f9248e014c7592df8de84ad6548a6fe3de2d805bb75fc/kubernetes-11.0.0-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (7.1.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (3.12.4)\n",
            "Collecting ml-pipelines-sdk==0.26.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/43/dd31442a07c808fcea65967a03154ffe9bdc2c297372ba7ca11c09bb7238/ml_pipelines_sdk-0.26.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.4MB/s \n",
            "\u001b[?25hCollecting docker<5,>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/22/410313ad554477e87ec406d38d85f810e61ddb0d2fc44e64994857476de9/docker-4.4.4-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 45.3MB/s \n",
            "\u001b[?25hCollecting keras-tuner<1.0.2,>=1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-hub<0.10,>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/83/a7df82744a794107641dad1decaad017d82e25f0e1f761ac9204829eef96/tensorflow_hub-0.9.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 51.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-cloud<0.2,>=0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/f5/0ac7277236d2d3ea66220a219c2b9a40ed34e0bfc3b387cbd7f745dcfb30/tensorflow_cloud-0.1.13-py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs<21,>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (20.3.0)\n",
            "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from tfx==0.26.1) (2.3.2)\n",
            "Collecting tensorflow-metadata<0.27,>=0.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/8f/4d28679c1c23237de615661d5d2961b9c79114f1a00b4daf3ccc451585f3/tensorflow_metadata-0.26.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<0.27,>=0.26.1->tfx==0.26.1) (1.1.5)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<0.27,>=0.26.1->tfx==0.26.1) (1.18.5)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.1) (1.26.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.1) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.1) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.1) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.1) (1.28.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3,>=2.7.3->tfx==0.26.1) (1.1.1)\n",
            "Collecting joblib<0.15,>=0.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 28.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform<0.27,>=0.26->tfx==0.26.1) (1.3.0)\n",
            "Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (1.4.1)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (7.6.3)\n",
            "Collecting ipython<8,>=7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/b1/82cbe2b856386f44f37fdae54d9b425813bd86fe33385c9d658d64826098/ipython-7.22.0-py3-none-any.whl (785kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (1.7)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (3.7.4.3)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (3.11.3)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (2018.9)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/c7/9ef9fda5e178aa5f5cda00c0d7505749506c540f9caf876d23c6cf915bf9/fastavro-1.3.5-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 29.1MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.5MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (2.8.1)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (4.1.3)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 45.9MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/17/536768bada8f93f124826b36dbdcdf08edd0e5ef0ca76b4c911f9f28596a/google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (1.21.0)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 50.3MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/c3/5b73c15f59207b20df288573c2ea203c7b126df8330add380d8b50bc0d5c/google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 27.7MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/7f/e10d602c2dc3f749f1b78377a3357790f1da71b28e7da9e5bc20b3a9bd40/google_cloud_vision-1.0.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 54.3MB/s \n",
            "\u001b[?25hCollecting google-cloud-build<3,>=2.0.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/04/b566b23f3fdea72326efce6aef09e523b6f8ef3058959672f673c41ffaca/google_cloud_build-2.0.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (1.8.0)\n",
            "Collecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/16/c9262ca40f3a278f38df9d21dece1ae01ee24f8ed29937bd1f066f908f9f/google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 46.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/29/8d06211102c87768dc34943d9c92abd8b67491ffedd6d09b56305b1ab255/google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (1.0.3)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 42.8MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (4.2.1)\n",
            "Collecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx==0.26.1) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx==0.26.1) (1.3.0)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/33/80e0d4f60e84a1ddd9a03f340be1065a2a363c47ce65c4bd3bae65ce9631/websocket_client-0.58.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx==0.26.1) (54.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx==0.26.1) (2020.12.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.1) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.1) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.1) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorboard>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (2.4.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (1.18.1)\n",
            "Collecting tensorflow-datasets<3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/41/8404fabb295025c1ced13bb150246f75770619889d8637afa523a4d09cdb/tensorflow_datasets-3.0.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (0.36.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.1) (1.6.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata<0.27,>=0.26->tfx-bsl<0.27,>=0.26.1->tfx==0.26.1) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx==0.26.1) (20.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client<2,>=1.7.8->tfx==0.26.1) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client<2,>=1.7.8->tfx==0.26.1) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2->tensorflow-transform<0.27,>=0.26->tfx==0.26.1) (2.4.7)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (5.1.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (5.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (1.0.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (4.10.1)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.18.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (4.4.2)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e6/4b4ca4fa94462d4560ba2f4e62e62108ab07be2e16a92e594e43b12d3300/prompt_toolkit-3.0.18-py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 47.7MB/s \n",
            "\u001b[?25hCollecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (0.6.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (0.4.8)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.1) (0.4.1)\n",
            "Collecting libcst>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/7f/4aa1419b0ecb8a31a79fef7a79b49e6a07b977baa6c94612aeeda0228d17/libcst-0.3.18-py3-none-any.whl (512kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 36.8MB/s \n",
            "\u001b[?25hCollecting proto-plus>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hCollecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/78/20/c862d765287e9e8b29f826749ebae8775bdca50b2cb2ca079346d5fbfd76/fasteners-0.16-py2.py3-none-any.whl\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx==0.26.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (0.4.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets<3.1.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (2.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (4.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (2.6.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (5.3.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (5.3.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.7.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.2.5)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1c/66402db44184904a2f14722d317a4da0b5c8c78acfc3faf74362566635c5/typing_inspect-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (3.8.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.9.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (1.5.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (22.0.3)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.3.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.1) (3.4.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (1.4.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.1) (0.5.1)\n",
            "Building wheels for collected packages: keras-tuner, dill, future, avro-python3, google-apitools, terminaltables, grpc-google-iam-v1\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp37-none-any.whl size=73200 sha256=01f1ca5b2fce232f49f10e76635b79574b0861b67f5056dcb50611d0c0ce1eaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78532 sha256=77f83e8c8dda37089fd6a2e6da10afed502c30350201831c497e3b36b3e6b805\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=beba9ad96ffcaac1549c450e0d1494c3474c102390f457021f5d702d852d6467\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp37-none-any.whl size=43516 sha256=4f94b7041e906e442a982ae051c79d5a95f2abc3715731fe724924c12e067a01\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp37-none-any.whl size=131043 sha256=6b836d0a981e6cbc9a8bfc4d9e93b73fca26c0133c71177932570e70c6b52576\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=1efcd9660381b5a6fcf48dc4150ed370c5e00a4c654357f4f9b0d6e4e6a42e75\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp37-none-any.whl size=18500 sha256=7cf27bc7a048e8e574391ff5c220792c409b178f7cd3c9dcff4d85876e083e83\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built keras-tuner dill future avro-python3 google-apitools terminaltables grpc-google-iam-v1\n",
            "\u001b[31mERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigtable 1.7.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: libcst 0.3.18 has requirement pyyaml>=5.2, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-spanner 1.19.1 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: absl-py, tensorflow-metadata, tensorflow-serving-api, pyarrow, pbr, mock, fastavro, requests, hdfs, dill, future, avro-python3, grpc-google-iam-v1, google-cloud-bigtable, google-cloud-pubsub, google-cloud-dlp, google-cloud-vision, mypy-extensions, typing-inspect, libcst, proto-plus, google-cloud-build, google-cloud-spanner, google-cloud-videointelligence, fasteners, google-apitools, google-cloud-language, grpcio-gcp, apache-beam, tfx-bsl, joblib, tensorflow-transform, tensorflow-data-validation, prompt-toolkit, ipython, tensorflow-model-analysis, ml-metadata, websocket-client, kubernetes, docker, ml-pipelines-sdk, terminaltables, colorama, keras-tuner, tensorflow-hub, tensorflow-datasets, tensorflow-cloud, tfx\n",
            "  Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Found existing installation: tensorflow-metadata 0.29.0\n",
            "    Uninstalling tensorflow-metadata-0.29.0:\n",
            "      Successfully uninstalled tensorflow-metadata-0.29.0\n",
            "  Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: dill 0.3.3\n",
            "    Uninstalling dill-0.3.3:\n",
            "      Successfully uninstalled dill-0.3.3\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: tensorflow-hub 0.11.0\n",
            "    Uninstalling tensorflow-hub-0.11.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.11.0\n",
            "  Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed absl-py-0.10.0 apache-beam-2.28.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 docker-4.4.4 fastavro-1.3.5 fasteners-0.16 future-0.18.2 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-build-2.0.0 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 ipython-7.22.0 joblib-0.14.1 keras-tuner-1.0.1 kubernetes-11.0.0 libcst-0.3.18 ml-metadata-0.26.0 ml-pipelines-sdk-0.26.1 mock-2.0.0 mypy-extensions-0.4.3 pbr-5.5.1 prompt-toolkit-3.0.18 proto-plus-1.18.1 pyarrow-0.17.1 requests-2.25.1 tensorflow-cloud-0.1.13 tensorflow-data-validation-0.26.0 tensorflow-datasets-3.0.0 tensorflow-hub-0.9.0 tensorflow-metadata-0.26.0 tensorflow-model-analysis-0.26.0 tensorflow-serving-api-2.3.0 tensorflow-transform-0.26.0 terminaltables-3.1.0 tfx-0.26.1 tfx-bsl-0.26.1 typing-inspect-0.6.0 websocket-client-0.58.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "absl",
                  "dill",
                  "google",
                  "prompt_toolkit",
                  "requests",
                  "tensorflow_hub"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-models-official==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/33/91e5e90e3e96292717245d3fe87eb3b35b07c8a2113f2da7f482040facdb/tf_models_official-2.3.0-py2.py3-none-any.whl (840kB)\n",
            "\r\u001b[K     |▍                               | 10kB 11.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 17.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 11.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 61kB 6.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 71kB 5.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |████                            | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 143kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 153kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 163kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 174kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 184kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 194kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 204kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 215kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 225kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 235kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 245kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 256kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 266kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 276kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 286kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 296kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 307kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 317kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 327kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 337kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 348kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 358kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 368kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 378kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 389kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 399kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 409kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 419kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 430kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 440kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 450kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 460kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 471kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 481kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 491kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 501kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 512kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 522kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 532kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 542kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 552kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 563kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 573kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 583kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 593kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 604kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 614kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 624kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 634kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 645kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 655kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 665kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 675kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 686kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 696kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 706kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 716kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 727kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 737kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 747kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 757kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 768kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 778kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 788kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 798kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 808kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 819kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 829kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 839kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 849kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (1.18.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (1.12.8)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (2.3.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (1.21.0)\n",
            "Collecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (1.5.12)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (3.0.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.0MB/s \n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (0.29.22)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 35.6MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/6d/92f377bece9b0ec9c893081dbe073a65b38d7ac12ef572b8f70554d08760/opencv_python_headless-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 123kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (0.9.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (0.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0) (3.13)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->tf-models-official==2.3.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->tf-models-official==2.3.0) (2.8.1)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0) (1.28.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0) (1.26.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0) (0.17.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (2.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->tf-models-official==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0) (0.4.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0) (2020.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0) (2.25.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.3.0) (0.26.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.3.0) (0.18.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.3.0) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.3.0) (0.3.1.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official==2.3.0) (20.3.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official==2.3.0) (2.7.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official==2.3.0) (0.1.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official==2.3.0) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official==2.3.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0) (54.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0) (20.9)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.3.0) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.3.0) (2.10)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official==2.3.0) (3.4.1)\n",
            "Building wheels for collected packages: py-cpuinfo\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp37-none-any.whl size=20070 sha256=7640161c96b66919bc571ab96f7429f477cdf67416a3a0de1502ef724c906580\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built py-cpuinfo\n",
            "Installing collected packages: sentencepiece, tf-slim, py-cpuinfo, tensorflow-addons, tensorflow-model-optimization, dataclasses, opencv-python-headless, tf-models-official\n",
            "Successfully installed dataclasses-0.6 opencv-python-headless-4.5.1.48 py-cpuinfo-7.0.0 sentencepiece-0.1.95 tensorflow-addons-0.12.1 tensorflow-model-optimization-0.5.0 tf-models-official-2.3.0 tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-pkgVxrxql4"
      },
      "source": [
        "* Use clothing data set to develop a pre-trained model \n",
        "  * Amazon product descriptions in metadata files: http://deepyeti.ucsd.edu/jianmo/amazon/index.html\n",
        "  * Read data from json files: http://jmcauley.ucsd.edu/data/amazon/ ; https://nijianmo.github.io/amazon/index.html\n",
        "\n",
        "* clothing/fashion word embeddings https://making.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
        "* https://github.com/mvasil/fashion-compatibility\n",
        "* general product descriptions data: https://data.world/promptcloud/fashion-products-on-amazon-com/workspace/file?filename=amazon_co-ecommerce_sample.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix8SGWXZROVX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn6f-mUfbN_J"
      },
      "source": [
        "* https://www.rexegg.com/regex-quickstart.html\n",
        "* https://docs.python.org/3/library/re.html\n",
        "* http://www.pyregex.com/\n",
        "* https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
        "* All Unicode characters https://en.wikipedia.org/wiki/List_of_Unicode_characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-J-IDUQM0W"
      },
      "source": [
        "### Cleaning functions\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ya8o9ZwQNCZ"
      },
      "source": [
        "example_text = \"\"\n",
        "\n",
        "tf.strings.lower(example_text)  # converts example_text to lowercase\n",
        "\n",
        "tf.strings.regex_replace(example_text, '<br />', '') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# string.punctuation contains !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~   # special characters like these are not removed\n",
        "# similar to string.punctuation functions https://docs.python.org/3/library/string.html\n",
        "# re.escape is to escape special characters in above\n",
        "# % is used to point to string.punctuation\n",
        "# Any punctuation attached to present seperately in the string is removed\n",
        "string.punctuation+'®' # to add more characters to string.punctuation\n",
        "tf.strings.regex_replace(example_text,'[%s]' % re.escape(string.punctuation),'') # Replaces line breaks with emptystring in example_text \n",
        "\n",
        "# To convert/encode unicode characters \n",
        "example_text.encode(\"utf-8\")\n",
        "# To decode the format back to unicode character\n",
        "example_text.dencode(\"utf-8\")\n",
        "\n",
        "# Extract only text data without labels\n",
        "raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# To get vocab size of the vectorize layer\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n",
        "\n",
        "# Identify all unicode characters\n",
        "[^\\u0000-\\u007e]+\n",
        "\n",
        "# Retains only numbers or letters\n",
        "Df['Text_col'].apply(lambda x: re.findall(\"\\w+\",str(x)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eBmLZ_exL14",
        "outputId": "a66092de-78a8-4b7c-bb55-a36371740c38"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "hub_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
        "embed = hub.KerasLayer(hub_url)\n",
        "embeddings = embed([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
        "print(embeddings.shape, embeddings.dtype)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 128) <dtype: 'float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98mmjDzxEuB1"
      },
      "source": [
        "### BERT\n",
        "* Difference between various BERT pre-trained models: https://tfhub.dev/google/collections/bert/1\n",
        "* L corresponds to number of hidden layers\n",
        "* H is hidden size\n",
        "* A is Attention heads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uH6rYlvF3uA"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "# from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDpF66S1EtPW",
        "outputId": "d526e18a-2665-40a8-9dfc-eda3e60c45f3"
      },
      "source": [
        "# BERT pre-process model\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "# BERT pre-trained model\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "\n",
        "text_test = ['this is such an amazing movie']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "# input_word_ids contains numerical ids for each tokenized input\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "\n",
        "# Max sequence length for any BERT model = 512\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')  # sequence length = 128\n",
        "\n",
        "# Inputs are tokenized using vocab file\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')  # Tokenized sequence\n",
        "\n",
        "# 1 for useful tokens, 0 for padding\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')  # Binary encoding to differentiate padded tokens\n",
        "\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')  # indicates whether the token i belongs to sentence 1 (value 0) or 2 (value 1)\n",
        "\n",
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "# outputs=[pooled_output, sequence_output])\n",
        "\n",
        "# sequence_output for 1 sentence is 512 dimensions for 128 tokens (as 128 is max sequence length)\n",
        "\n",
        "# Each position outputs a vector of 512. Outputs classification CLS token\n",
        "# We are interested in only CLS token to pass through further keras layer for classification/regression.\n",
        "# pooled_output shape of CLS token is [batch_size, 512]  \n",
        "\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  102    0    0    0    0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 0 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.94622993  0.96978396  0.13774446  0.27936164 -0.3334526   0.34829125\n",
            "  0.94912225 -0.9515633  -0.00178645 -0.93107945 -0.04124452 -0.97975695]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[-0.34628797  0.25210583  0.5756581  ... -0.15549679  0.472016\n",
            "   0.37473977]\n",
            " [-0.5864688   0.31059366 -0.27019346 ...  0.5594992  -0.23402517\n",
            "   0.92162156]\n",
            " [-0.87764287  0.43894887 -0.61759305 ...  0.0450628  -0.34966043\n",
            "   0.43843132]\n",
            " ...\n",
            " [-0.3622594  -0.2679375  -0.04834296 ...  0.40202618  0.5345558\n",
            "   0.5671515 ]\n",
            " [-1.0682071  -0.670603    0.14062242 ...  0.12432958  0.40765905\n",
            "   0.7977506 ]\n",
            " [-0.42452416 -0.14097393  0.02649997 ...  0.1602087   0.8911413\n",
            "  -0.0625942 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRRaVmqd6MpC",
        "outputId": "0da2293e-83cc-4d4a-d458-09aef8d721bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow_text as text\n",
        "\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "# BERT pre-trained model\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "feature_vec_1 = bert_model(bert_preprocess_model([\"3/4 sleeve\"]))[\"pooled_output\"]\n",
        "feature_vec_2 = bert_model(bert_preprocess_model([\"3 4 sleeve\"]))[\"pooled_output\"]\n",
        "\n",
        "cosine_similarity(feature_vec_1, feature_vec_2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337ac9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337ac9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337a10e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7337a10e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 12 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7368c2dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 12 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd7368c2dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zRClgA-8LRG",
        "outputId": "c5d0e310-95f2-4675-c1dd-f5064decabc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow_text as text\n",
        "\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "# BERT pre-trained model\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cosine_similarity_fn(text1,text2):\n",
        "  feature_vec_1 = bert_model(bert_preprocess_model([text1]))[\"pooled_output\"]\n",
        "  feature_vec_2 = bert_model(bert_preprocess_model([text2]))[\"pooled_output\"]\n",
        "  return cosine_similarity(feature_vec_1, feature_vec_2)\n",
        "\n",
        "print(cosine_similarity_fn(\"3/4 sleeve\",\"3 4 sleeve\"))\n",
        "print(cosine_similarity_fn(\"3/4 sleeve\",\"3 4 sleeve\"))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd733661a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd733661a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd731864c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd731864c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd733661a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd733661a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9320507]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KkUWaj0Aq2q"
      },
      "source": [
        "def cosine_similarity_fn(text1,text2):\n",
        "  feature_vec_1 = bert_model(bert_preprocess_model([text1]))[\"pooled_output\"]\n",
        "  feature_vec_2 = bert_model(bert_preprocess_model([text2]))[\"pooled_output\"]\n",
        "  return 'similarity of' , text1,'and', text2, cosine_similarity(feature_vec_1, feature_vec_2)\n",
        "  return \"similarity of %s and %s\" % (text1,text2), text1, \" and \", text2, cosine_similarity(feature_vec_1, feature_vec_2)\n",
        "# cosine_similarity_fn(\"3/4 sleeve\",\"3 4 sleeve\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uiPumczER9c"
      },
      "source": [
        "def preprocessing_fn(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  linebreaks = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  stripped_html = tf.strings.regex_replace(linebreaks,\n",
        "                                  '[%s]' % re.escape(string.punctuation),'')\n",
        "  return tf.strings.regex_replace(stripped_html, '[^\\u0000-\\u007e]+', '')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZIKnCHdGBzC",
        "outputId": "fee11bd2-7cdf-457e-89ac-4f961e3ccee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(cosine_similarity_fn(\"don't\",\"do not\"))\n",
        "print(cosine_similarity_fn(\"they're\",\"they are\"))\n",
        "print(cosine_similarity_fn(\"it's\",\"it is\"))\n",
        "print(cosine_similarity_fn(\"it?s\",\"it is\"))\n",
        "print(cosine_similarity_fn(\"easy-to-layer\",\"easy to layer\"))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('similarity of', \"don't\", 'and', 'do not', array([[0.9083276]], dtype=float32))\n",
            "('similarity of', \"they're\", 'and', 'they are', array([[0.8668688]], dtype=float32))\n",
            "('similarity of', \"it's\", 'and', 'it is', array([[0.76479155]], dtype=float32))\n",
            "('similarity of', 'it?s', 'and', 'it is', array([[0.65618396]], dtype=float32))\n",
            "('similarity of', 'easy-to-layer', 'and', 'easy to layer', array([[0.89126873]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V41Sk_K6ENuY",
        "outputId": "3c7f32f5-aebb-4438-e506-7e04f4ca51f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(cosine_similarity_fn(preprocessing_fn(\" Standard size 1.7 fl oz/ 30 ml.\"),\" Standard size 1.7 fl oz/ 30 ml.\"))\n",
        "print(cosine_similarity_fn(\" Standard size 1.7 fl oz  30 ml.\",\" Standard size 1.7 fl oz/ 30 ml.\"))\n",
        "print(cosine_similarity_fn(\"don't\",\"do not\"))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('similarity of ', <tf.Tensor: shape=(), dtype=string, numpy=b' standard size 17 fl oz 30 ml'>, ' and ', ' Standard size 1.7 fl oz/ 30 ml.', array([[0.6451631]], dtype=float32))\n",
            "('similarity of ', ' Standard size 1.7 fl oz  30 ml.', ' and ', ' Standard size 1.7 fl oz/ 30 ml.', array([[0.9953242]], dtype=float32))\n",
            "('similarity of ', \"don't\", ' and ', 'do not', array([[0.9083276]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W00MreBS-ylQ",
        "outputId": "2e6ab98f-9d7e-4833-9cda-107c954f340e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(cosine_similarity_fn(\"3/4 sleeve\",\"3 4 sleeve\"))\n",
        "print(cosine_similarity_fn(\"3/4 sleeve\",\"3 by 4 sleeve\"))\n",
        "\n",
        "\n",
        "print(cosine_similarity_fn(\"3 ®\",\"3 \\xc2\\xae\"))\n",
        "print(cosine_similarity_fn(\"3®\",\"3\\xc2\\xae\"))\n",
        "print(cosine_similarity_fn(\"3®\",\"3\"))\n",
        "print(cosine_similarity_fn(\"3 ®\",\"3®\"))\n",
        "print(cosine_similarity_fn(\"A/B\",\"A by B\"))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('similarity of ', '3/4 sleeve', ' and ', '3 4 sleeve', array([[0.9320507]], dtype=float32))\n",
            "('similarity of ', '3/4 sleeve', ' and ', '3 by 4 sleeve', array([[0.96390617]], dtype=float32))\n",
            "('similarity of ', '3 ®', ' and ', '3 Â®', array([[0.87759125]], dtype=float32))\n",
            "('similarity of ', '3®', ' and ', '3Â®', array([[0.9169636]], dtype=float32))\n",
            "('similarity of ', '3®', ' and ', '3', array([[0.8828571]], dtype=float32))\n",
            "('similarity of ', '3 ®', ' and ', '3®', array([[0.8762013]], dtype=float32))\n",
            "('similarity of ', 'A/B', ' and ', 'A by B', array([[0.8666401]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPV7HaEqCrvF",
        "outputId": "7c446405-55ff-435f-c1c9-588affe28208",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"®\".encode())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\\xc2\\xae'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdHmx0dm-ydc",
        "outputId": "752c74ba-687e-407c-9902-7d696520a779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "string_check = \"� � 3® ° 200gsm (Bright Lycra)\"\n",
        "string_check.encode(\"utf-8\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'\\xef\\xbf\\xbd \\xef\\xbf\\xbd 3\\xc2\\xae \\xc2\\xb0 200gsm (Bright Lycra)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJxI_20e-xyK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OVOB6Kf6Yu9"
      },
      "source": [
        "text_test = ['®!#$%&()*+,-./:;<=>?@[\\]^_`{|}~®'] \n",
        "# text_test = ['\\xef\\xbf\\xbd'] #�"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-ZrdcQbqX8"
      },
      "source": [
        "### Activation functions for Regression(y>0) :\n",
        "* softplus\n",
        "* ReLU\n",
        "* Exponential\n",
        "* Piece-wise linear\n",
        "\n",
        "### loss functions\n",
        "* Region Proposal Network box\n",
        "* object Localization Network RPN centerness "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd4XUXUuP-gV"
      },
      "source": [
        "### https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "* In Text Vectorize layer: Each word is given a unique number and encoding of the sentence of the sentence is based on that. Transforms strings into vocab indices\n",
        "* This Vectorize layer is passed through the keras embedding layer to get embedding vectors for those words. Embedding vector is learnt during training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ-fBZYxP4So"
      },
      "source": [
        "### Tensorflow Notebooks\n",
        "\"\"\" Data is extracted from url into folders. Then data is passed through 'tf.keras.preprocessing.text_dataset_from_directory' function to \n",
        "prepare data into suitable format \"\"\"\n",
        "\n",
        "# Function for a sneak peek into the data </br>\n",
        "for text_batch, label_batch in raw_train_ds.take(1): \n",
        "  for i in range(3): \n",
        "    print(\"Review\", text_batch.numpy()[i]) \n",
        "    print(\"Label\", label_batch.numpy()[i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLXQVMd8RB4L",
        "outputId": "9422d6df-b1fa-4cc7-edb6-fe54fe1ca364"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', '')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')\n",
        "\n",
        "# Added removal of unicode characters to the above code  \n",
        "def custom_standardization_check(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  linebreaks = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  stripped_html = tf.strings.regex_replace(linebreaks,\n",
        "                                  '[%s]' % re.escape(string.punctuation),' ')\n",
        "  return tf.strings.regex_replace(stripped_html, '[^\\u0000-\\u007e]+', ' ')\n",
        "\n",
        "raw_data_sample = 'ത'+'\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
        "custom_standardization(raw_data_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xe0\\xb4\\xa4pandemonium is a horror\\xc2\\xae movie spoof  that comes off more stupid than funnybelieve me when i tell you i love comedies especially comedy spoofs airplane the naked gun trilogy blazing saddles high anxiety and spaceballs are some of my favorite comedies that spoof a particular genre pandemonium is not up there with those films most of the scenes in this movie had me sitting there in stunned silence because the movie wasnt all that funny there are a few laughs in the film but when you watch a comedy you expect to laugh a lot more than a few times and thats all this film has going for it geez scream had more laughs than this film and that was more of a horror film how bizarre is that12 out of four'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJlVDEO3oImm"
      },
      "source": [
        "# ADAPT the vectorize_layer on only Text part of train dat (remove label)\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,  # Default is 'lower_and_strip_punctuation'.\n",
        "    max_tokens=None,  # no cap on number of tokens\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call adapt\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lqZhvbpeW-"
      },
      "source": [
        "# To retrieve a batch (of 32 reviews and labels) from the dataset: loop is added to the existing code\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "for i in range(len(text_batch)):\n",
        "  first_review, first_label = text_batch[i], label_batch[i]\n",
        "  print(\"Review\", first_review)\n",
        "  print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "  print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO5QqSueqcSj"
      },
      "source": [
        "# To get word associated with a unique number\n",
        "vectorize_layer.get_vocabulary()[1287]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_bnIsIejse"
      },
      "source": [
        "text_col = data['feature_text'].values\n",
        "target_col = data['target'].values\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=None,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=None)\n",
        "\n",
        "vectorize_layer.adapt(text_col) \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  vectorize_layer, # Added Vectorize layer here\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  tf.keras.layers.LSTM(64),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  Dense(1,activation='softplus')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "              metrics=[tf.keras.metrics.MeanAbsolutePercentageError(), tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNyUShZSiojz"
      },
      "source": [
        "# Extract all unciode characters from text column and then concatenate all characters in differnt rows of a column as a string\n",
        "df['unicode_chars'] = df['Text_col'].apply(lambda x: re.findall('[^\\u0000-\\u007e]+',str(x)))\n",
        "df['unicode_chars'] = [','.join(map(str, l)) for l in df['unicode_chars']]  # concatenate rows as a string\n",
        "unicode_chars_uq = set(', '.join(df['unicode_chars'].apply(lambda x: str(x))))  # to get distinct characters\n",
        "unicode_chars_uq\n",
        "# result = {' ', ',', '\\xa0', '®', '°', 'º', '–', '—', '’', '”', '™', '�'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roi6vXhRzi-c",
        "outputId": "378e512c-7403-4e67-960b-6c7d381f86d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "string_check = \"sandy� � ® ° 200gsm (Bright Lycra)\"\n",
        "string_check.encode(\"utf-8\")\n",
        "# print(string_check.encode('ascii', 'replace'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'sandy\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xc2\\xae \\xc2\\xb0 200gsm (Bright Lycra)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "nk5qlQTgZE08",
        "outputId": "9c041f79-3117-448d-d5c7-adbeb1dd7be3"
      },
      "source": [
        "raw_data_sample = 'ത'+'\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
        "\n",
        "print(raw_data_sample)\n",
        "print(('ത'+raw_data_sample).encode(\"utf-8\"))\n",
        "print(raw_data_sample.encode(\"utf-8\").decode(\"utf-8\"))\n",
        "\n",
        "# Identify all unicode characters\n",
        "df['text_col'].apply(lambda x: re.findall('[^\\u0000-\\u007e]+',str(x)))\n",
        "\n",
        "string_check = \"sandy� � ® ° 200gsm (Bright Lycra)\"\n",
        "print(string_check.encode('ascii', 'replace'))\n",
        "Df['text_col'].apply(lambda x: x.encode('ascii', 'replace'))\n",
        "[s.encode('ascii', 'ignore') for s in Df['text_col']]\n",
        "\n",
        "print(string_check.encode('ascii', 'ignore'))\n",
        "string_check\n",
        "\n",
        "# Tensorflow \n",
        "# converted � to ? {' ', ',', '\\xa0', '®', '°', 'º', '–', '—', '’', '”', '™', '�'}\n",
        "string_check = tf.constant(u\"sandy� � ® ° 200gsm (Bright Lycra)\".encode('ascii','ignore'))\n",
        "print(string_check)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ത\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
            "b'\\xe0\\xb4\\xa4\\xe0\\xb4\\xa4\"Pandemonium\" -is a horror\\xc2\\xae movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "ത\"Pandemonium\" -is a horror® movie spoof - !that comes off more stupid than funny.<br /><br />Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn't all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that's all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)\n",
            "b'sandy? ? ? ? 200gsm (Bright Lycra)'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-751468cb2dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstring_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sandy� � ® ° 200gsm (Bright Lycra)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_col'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_col'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erHhXAxXejpV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH0AtmOLejmL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRwGrTxeejjK"
      },
      "source": [
        "self.embedding = nn.Embedding(one_hot_dim, embedding_dim)\n",
        "        self.maxpool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool1d(kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, stride=stride_size, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(in_features=size_after_conv, out_features=output_dim)\n",
        "\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvSpqgY-RC6G",
        "outputId": "2ba77aa1-f9b8-4216-a4c3-759e04f84fa4"
      },
      "source": [
        "re.findall(r'[%s]', raw_data_sample)\n",
        "print(re.findall(r'[%s] % re.escape(string.punctuation)', raw_data_sample))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uTZpwwVfB_U"
      },
      "source": [
        " \"résumé\".encode(\"utf-8\")\n",
        "\n",
        " \"El Niño\".encode(\"utf-8\")\n",
        "\n",
        "\n",
        " b\"r\\xc3\\xa9sum\\xc3\\xa9\".decode(\"utf-8\")\n",
        "'résumé'\n",
        ".decode(\"utf-8\")\n",
        "'El Niño'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l0TIGwO8YUp_",
        "outputId": "e617804f-b0a7-4caa-83e2-6330bafc65a3"
      },
      "source": [
        "import emoji\n",
        "\n",
        "def extract_emojis(s):\n",
        "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\n",
        "\n",
        "extract_emojis(raw_data_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHbSF8z7Y0m5",
        "outputId": "39ff3db1-64f2-4742-a3ef-266c4d9a0da8"
      },
      "source": [
        "pattern = re.compile(\"d\")\n",
        "pattern.search(\"dog\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='d'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}